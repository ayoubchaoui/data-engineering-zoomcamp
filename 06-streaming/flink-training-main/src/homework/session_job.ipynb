{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08de972-b138-42c1-bd25-4a98166c68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, DataTypes, TableEnvironment, StreamTableEnvironment\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.common.time import Duration\n",
    "\n",
    "def create_events_aggregated_sink(t_env):\n",
    "    table_name = 'processed_events_aggregated'\n",
    "    sink_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            event_hour TIMESTAMP(3),\n",
    "            test_data INT,\n",
    "            num_hits BIGINT,\n",
    "            PRIMARY KEY (event_hour, test_data) NOT ENFORCED\n",
    "        ) WITH (\n",
    "            'connector' = 'jdbc',\n",
    "            'url' = 'jdbc:postgresql://postgres:5432/postgres',\n",
    "            'table-name' = '{table_name}',\n",
    "            'username' = 'postgres',\n",
    "            'password' = 'postgres',\n",
    "            'driver' = 'org.postgresql.Driver'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(sink_ddl)\n",
    "    return table_name\n",
    "# test data :dict \n",
    "def create_events_source_kafka(t_env):\n",
    "    table_name = \"events\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            lpep_pickup_datetime TIMESTAMP(3),\n",
    "            lpep_dropoff_datetime TIMESTAMP(3),\n",
    "            PULocationID INTEGER,\n",
    "            DOLocationID INTEGER,\n",
    "            passenger_count INTEGER,\n",
    "            trip_distance DOUBLE,\n",
    "            tip_amount DOUBLE,\n",
    "            WATERMARK FOR lpep_dropoff_datetime AS lpep_dropoff_datetime - INTERVAL '5' SECOND\n",
    "           \n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'test-topic',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name\n",
    "\n",
    "\n",
    "def log_aggregation():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.enable_checkpointing(10 * 1000)\n",
    "    env.set_parallelism(3)\n",
    "\n",
    "    # Set up the table environment\n",
    "    settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "    t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "\n",
    "    watermark_strategy = (\n",
    "        WatermarkStrategy\n",
    "        .for_bounded_out_of_orderness(Duration.of_seconds(5))\n",
    "        .with_timestamp_assigner(\n",
    "            # This lambda is your timestamp assigner:\n",
    "            #   event -> The data record\n",
    "            #   timestamp -> The previously assigned (or default) timestamp\n",
    "            lambda event, timestamp: event[2]  # We treat the second tuple element as the event-time (ms).\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        # Create Kafka table\n",
    "        source_table = create_events_source_kafka(t_env)\n",
    "        aggregated_table = create_events_aggregated_sink(t_env)\n",
    "\n",
    "        t_env.execute_sql(f\"\"\"\n",
    "        INSERT INTO {aggregated_table}\n",
    "        SELECT\n",
    "            window_start AS session_start,\n",
    "            window_end AS session_end,\n",
    "            PULocationID,\n",
    "            DOLocationID,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM TABLE(\n",
    "            SESSION(TABLE {source_table}, DESCRIPTOR(lpep_dropoff_datetime), INTERVAL '5' MINUTES)\n",
    "        )\n",
    "        GROUP BY window_start, window_end, PULocationID, DOLocationID;\n",
    "        \n",
    "        \"\"\").wait()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Writing records from Kafka to JDBC failed:\", str(e))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_aggregation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
